\chapter{Differential-Algebraic Equations}

Engineering problems often contain combinations of differential and algebraic equations. Differential equations describe the evolution of states (how things change over time), and algebraic equations can appear to enforce constraints, such as the conservation of energy, mass, momentum, atomic species, or charge, on the state variables. Moreover, algebraic equations may represent pseudo-steady-state approximations for fast dynamics or imposed control objectives. When we mix these two types of equations, we obtain a system of differential-algebraic equations (DAEs). To formalize this, we define a state vector $\mathbf{z}(t)$ that contains all the variables of interest. The most general governing equation for a DAE system is written as a fully implicit function of time, the state, and the time-derivative of the state:
\begin{equation}
    \mathbf{f}(t, \mathbf{z}, \dot{\mathbf{z}}) = \mathbf{0}, \qquad \text{where } \dot{\mathbf{z}} := \frac{d\mathbf{z}}{dt}
\end{equation}
This looks similar to the implicit ODE formulations we have seen previously, but there is a difference in the nature of the Jacobian with respect to $\dot{\mathbf{z}}$. For implicit ODEs, the Jacobian $\partial \mathbf{f} / \partial \dot{\mathbf{z}}$ is assumed to be nonsingular (locally invertible), allowing us to solve for $\dot{\mathbf{z}}$. For DAEs, this Jacobian may be singular, preventing a direct solution for $\dot{\mathbf{z}}$ without also using algebraic constraints. We will explore these analytical properties here and discuss numerical solutions later. Solving them numerically is very similar to solving ODE-IVPs.

\paragraph*{Motivation: Reaction Kinetics and Conservation}

To see why DAEs naturally arise, let's consider the chemical reaction network $A \longrightarrow B \rightleftharpoons 2D$. If we model this using standard kinetics, we obtain a system of ODEs describing the rate of change for each species concentration:
\begin{align}
    \frac{dC_A}{dt} &= -k_1 C_A \\
    \frac{dC_B}{dt} &= k_1 C_A - k_2 C_B + k_3 C_D^2 \\
    \frac{dC_D}{dt} &= 2k_2 C_B - 2k_3 C_D^2
\end{align}
In this ODE system, the total mass is implicitly conserved by the kinetics, but numerical integration errors might cause the total mass to drift over time during a simulation.  Alternatively, we might formulate this as a DAE system. We keep the kinetic rate laws for $A$ and $B$, but we replace the differential equation for $D$ with an algebraic constraint enforcing the exact conservation of atomic species (mass balance). If $Q_2$ is a constant total mass invariant (i.e., $Q_2 = \text{const.}$), the system becomes
\begin{align}
    \frac{dC_A}{dt} &= -k_1 C_A \\
    \frac{dC_B}{dt} &= k_1 C_A - k_2 C_B + k_3 C_D^2 \\
    Q_2 &= C_A + C_B + \frac{1}{2} C_D
\end{align}
In this DAE formulation, the exact conservation of mass is enforced directly by the algebraic equation. This prevents the drift that might occur in the pure ODE system.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/dae/dae_reaction.pdf}
    \caption{ODE vs.\ DAE simulation of the reaction network $A \to B \rightleftharpoons 2D$. Top: species concentrations from a forward-Euler time discretization ($\Delta t=0.05$) with $k_1=1.0$, $k_2=0.6$, $k_3=2.0$ and $(C_A,C_B,C_D)(0)=(1,0,0)$. In the ODE formulation, $C_A$, $C_B$, and $C_D$ are advanced by their kinetic rate laws; in the DAE formulation, only $C_A$ and $C_B$ are integrated and $C_D$ is recovered each step by directly enforcing the mass balance constraint $Q_2 = C_A + C_B + \tfrac{1}{2}C_D$ (with $Q_2=Q_2(0)$). Bottom: mass-balance violation $Q_2(t)-Q_2(0)$; rounding each Euler update to $10^{-3}$ induces visible drift for the ODE, whereas the DAE remains exactly on the constraint manifold.}
    \label{fig:dae_reaction}
\end{figure}

\section{Standard Forms of DAEs}

We can categorize DAEs into three common forms. The first, as introduced above, is the \textbf{fully implicit form}:
\begin{equation}
    \mathbf{f}(t, \mathbf{z}, \dot{\mathbf{z}}) = \mathbf{0}
\end{equation}

In many physical systems, the variables naturally separate into those that appear with time derivatives (differential states, $\mathbf{x}(t)$) and those that do not (algebraic states, $\mathbf{y}(t)$). This leads to the \textbf{semi-explicit form}:
\begin{equation}
    \begin{aligned}
        \dot{\mathbf{x}} &= \mathbf{f}(t, \mathbf{x}, \mathbf{y}) \\
        \mathbf{0} &= \mathbf{g}(t, \mathbf{x}, \mathbf{y})
    \end{aligned}
    \qquad \text{with } \mathbf{z}(t) = \begin{bmatrix} \mathbf{x}(t) \\ \mathbf{y}(t) \end{bmatrix}
\end{equation}
It is often convenient to write equations in this manner, as many physical balances naturally give us this structure. Interestingly, the semi-explicit form can be rewritten trivially in the third form, the \textbf{mass matrix form}, by introducing a singular mass matrix:
\begin{equation}
    \begin{bmatrix} \mathbf{I} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix} \begin{bmatrix} \dot{\mathbf{x}} \\ \dot{\mathbf{y}} \end{bmatrix} = \begin{bmatrix} \mathbf{f}(t, \mathbf{x}, \mathbf{y}) \\ \mathbf{g}(t, \mathbf{x}, \mathbf{y}) \end{bmatrix}
\end{equation}
More generally, the mass matrix form is defined as
\begin{equation}
    \mathbf{M}(t, \mathbf{z})\dot{\mathbf{z}} = \mathbf{f}(t, \mathbf{z})
\end{equation}
Recall the standard ODE formulation $\dot{\mathbf{x}} = \mathbf{f}(t, \mathbf{x})$. If the matrix $\mathbf{M}$ is invertible, we can simply multiply by $\mathbf{M}^{-1}$ to rewrite the DAE as an ODE:
\begin{equation}
    \dot{\mathbf{z}} = \mathbf{M}(t, \mathbf{z})^{-1}\mathbf{f}(t, \mathbf{z})
\end{equation}
However, in the context of DAEs, the matrix $\mathbf{M}$ is most often singular (as seen in the semi-explicit block matrix example above). Consequently, we cannot convert the system to an ODE just by inverting $\mathbf{M}$. 

Regarding numerical solutions, MATLAB's \texttt{ode15s} and \texttt{ode23t} solvers can handle DAEs in mass matrix form. Additionally, MATLAB's \texttt{ode15i} solver is designed to solve fully implicit forms provided the DAEs are of index 1 (a concept we will define shortly).

\paragraph*{Example: Fluctuating Tank Feed}

Let us solidify these concepts with a concrete example of a fluctuating tank feed. Consider a stirred tank with an inlet concentration $c_1(t)$ and an outlet concentration $c_2(t)$. The volume $V$ and flow rate $Q$ are constant.

\begin{figure}[H]
    \centering
    % sorry for the compression... will fix later
    \includegraphics[width=0.15\textwidth]{figs/dae/tank1.png}
    \caption{A continuous stirred tank reactor with a fluctuating inlet concentration $c_1(t)$.}
    \label{fig:tank_dae}
\end{figure}

The governing equations for this system are the component balance for the tank and the forcing function for the inlet:
\begin{align}
    \frac{dc_2}{dt} &= \frac{Q}{V}(c_1(t) - c_2(t)) \\
    c_1(t) &= \gamma(t)
\end{align}
subject to initial conditions $c_1(0) = \gamma(0)$ and $c_2(0) = c_0$. We can derive an analytical solution for this system. The inlet concentration is simply the forcing function itself, $c_1(t) = \gamma(t)$. For the tank concentration, we solve the linear ODE using an integrating factor to obtain
\begin{equation}
    c_2(t) = c_0 e^{-(Q/V)t} + \frac{Q}{V} \int_0^t \gamma(t') e^{-(Q/V)(t-t')} dt'
\end{equation}
To analyze this system as a DAE, we must identify our states. Which are differential and which are algebraic? Here, $c_2$ is a differential state because its time derivative appears in the governing equations. $c_1$ is an algebraic state because no time derivative of $c_1$ appears; its value is determined instantaneously by the algebraic constraint $c_1(t) = \gamma(t)$.

We can express this system in our standard DAE forms. In the fully implicit form, we move everything to one side to satisfy $\mathbf{f}(t, \mathbf{z}, \dot{\mathbf{z}}) = \mathbf{0}$:
\begin{equation}
    \begin{bmatrix}
    \frac{dc_2}{dt} - \frac{Q}{V}(c_1(t) - c_2(t)) \\
    c_1(t) - \gamma(t)
    \end{bmatrix} = \mathbf{0}
\end{equation}

Alternatively, we can write this in the mass matrix form, $\mathbf{M}(t, \mathbf{z})\dot{\mathbf{z}} = \mathbf{f}(t, \mathbf{z})$. To do so, we first define our state vector $\mathbf{c}$ containing both algebraic and differential variables:
\begin{equation}
    \mathbf{c}(t) = \begin{bmatrix} c_1(t) \\ c_2(t) \end{bmatrix}
\end{equation}
Next, we rearrange our governing equations to explicitly display the coefficients for the time derivatives ($\dot{c}_1, \dot{c}_2$) on the left and the state variables ($c_1, c_2$) on the right. Note that since $c_1$ is algebraic, its derivative $\dot{c}_1$ has a coefficient of 0 in both equations:
\begin{equation}
    \begin{aligned}
        0 \cdot \dot{c}_1 + 1 \cdot \dot{c}_2 &= \frac{Q}{V} c_1 - \frac{Q}{V} c_2 \\
        0 \cdot \dot{c}_1 + 0 \cdot \dot{c}_2 &= -1 \cdot c_1 + 0 \cdot c_2 + \gamma(t)
    \end{aligned}
\end{equation}
By collecting these coefficients into matrices, we arrive at the linear mass matrix form:
\begin{equation}
    \underbrace{\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}}_{\mathbf{M}} 
    \underbrace{\begin{bmatrix} \dot{c}_1 \\ \dot{c}_2 \end{bmatrix}}_{\dot{\mathbf{c}}} 
    = 
    \underbrace{\begin{bmatrix} \frac{Q}{V} & -\frac{Q}{V} \\ -1 & 0 \end{bmatrix}}_{\mathbf{A}} 
    \underbrace{\begin{bmatrix} c_1 \\ c_2 \end{bmatrix}}_{\mathbf{c}} 
    + 
    \underbrace{\begin{bmatrix} 0 \\ \gamma(t) \end{bmatrix}}_{\mathbf{b}(t)}
\end{equation}
Note the singularity of the mass matrix $\mathbf{M}$ on the left-hand side (specifically, the row of zeros). This explicitly confirms that the system is a DAE since $\mathbf{M}$ cannot be inverted to recover a standard ODE.

\paragraph*{Example 2: Specified Tank Concentration}

Let us reconsider the stirred tank reactor from the previous section (\autoref{fig:tank_dae}). In the first example, we specified the input concentration and solved for the output. Now, consider the inverse problem: we wish to specify the \textit{output} concentration profile $c_2(t) = \gamma(t)$ and determine the necessary input concentration $c_1(t)$ to achieve it. The governing equations are still the component balance and the constraint, but the constraint now applies to the outlet variable:
\begin{align}
    \frac{dc_2}{dt} &= \frac{Q}{V}(c_1(t) - c_2(t)) \\
    c_2(t) &= \gamma(t)
\end{align}
The initial conditions must be consistent with the constraints that $c_2(0) = \gamma(0)$ and $c_1(0) = \gamma(0) + \frac{V}{Q}\dot{\gamma}(0)$.

Analytically, we can solve this by substituting the constraint directly into the differential equation. Since $c_2(t) = \gamma(t)$, it follows that $\frac{dc_2}{dt} = \dot{\gamma}(t)$. Substituting these into the component balance and rearranging for $c_1(t)$ gives the analytical solution:
\begin{align}
    c_1(t) &= \gamma(t) + \frac{V}{Q}\dot{\gamma}(t) \\
    c_2(t) &= \gamma(t)
\end{align}
There is a big difference here compared to the previous example. The solution for $c_1(t)$ depends on the \textit{derivative} of the forcing function $\dot{\gamma}(t)$. This dependence on derivatives is characteristic of higher-index DAEs, which we will define formally in a moment.

We can also write this system in our standard forms. The fully implicit form is
\begin{equation}
    \begin{bmatrix}
    \frac{dc_2}{dt} - \frac{Q}{V}(c_1(t) - c_2(t)) \\
    c_2(t) - \gamma(t)
    \end{bmatrix} = \mathbf{0}
\end{equation}
In mass matrix form, we again define the state vector as $\mathbf{c} = [c_1, c_2]^\top$. We rearrange the governing equations to isolate the time derivative terms on the left and the state terms on the right. For the component balance, $\dot{c}_2$ appears with a coefficient of 1. For the constraint equation, no time derivatives appear, so both coefficients are 0:
\begin{equation}
    \begin{aligned}
        0 \cdot \dot{c}_1 + 1 \cdot \dot{c}_2 &= \frac{Q}{V} c_1 - \frac{Q}{V} c_2 \\
        0 \cdot \dot{c}_1 + 0 \cdot \dot{c}_2 &= 0 \cdot c_1 - 1 \cdot c_2 + \gamma(t)
    \end{aligned}
\end{equation}
Collecting these terms into matrix notation,
\begin{equation}
    \underbrace{\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}}_{\mathbf{M}} 
    \underbrace{\begin{bmatrix} \dot{c}_1 \\ \dot{c}_2 \end{bmatrix}}_{\dot{\mathbf{c}}} 
    = 
    \underbrace{\begin{bmatrix} \frac{Q}{V} & -\frac{Q}{V} \\ 0 & -1 \end{bmatrix}}_{\mathbf{A}} 
    \underbrace{\begin{bmatrix} c_1 \\ c_2 \end{bmatrix}}_{\mathbf{c}} 
    + 
    \underbrace{\begin{bmatrix} 0 \\ \gamma(t) \end{bmatrix}}_{\mathbf{b}(t)}
\end{equation}
As before, the second row of the mass matrix is zero because the second equation is purely algebraic ($0 = -c_2 + \gamma(t)$).

\section{The Differential Index}

The behavior we observed above, where the solution depends on the derivative of the input, leads us to the concept of the \textbf{differential index}. The index of a DAE is defined as the number of times the system must be differentiated to obtain a complete set of first-order ODEs for all states (both algebraic and differential). To understand this definition, we look at the procedure of differentiating the algebraic constraints until we can express the time derivative of every state variable, $\dot{\mathbf{z}}$, as a function of the states and inputs.

\subsection{Index 1 vs.\ Index 2: The Tank Filling Problem}

The formulation of the DAE matters significantly for the numerical properties of the system. To illustrate this, let us examine a mechanical tank filling problem (distinct from the chemical concentration problem above). We define two variables, the fluid volume in the tank $x_1(t)$ and the outlet flow rate $x_2(t)$. The inlet flow rate is $g_1(t)$.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
    >={Latex[length=3mm]},
    tank/.style={line width=1.1pt, draw=black},
    flow/.style={->, line width=1.1pt},
    ]

        % --- geometry parameters ---
        \def\R{2.2}   % half-width of tank
        \def\E{0.55}  % ellipse "depth" (vertical radius of ellipses)
        \def\H{3.6}   % tank height
        \def\hw{1.8}  % water surface depth from the top (0..H)

        % Key y-levels
        \coordinate (Top) at (0,0);
        \coordinate (Bot) at (0,-\H);
        \coordinate (Water) at (0,-\hw);

        % --- clip to the tank interior (for filling water) ---
        \begin{scope}
        \path[clip]
            (-\R,0) arc (180:360:{\R} and {\E}) -- (\R,-\H)
            arc (0:-180:{\R} and {\E}) -- cycle;

        % Water fill
        \fill[blue!25] (-\R,-\hw) -- (-\R,-\H) arc (180:360:{\R} and {\E}) -- (\R,-\hw) arc (0:180:{\R} and {\E}) -- cycle;

        \end{scope}

        % --- tank outline (3D-ish cylinder) ---
        % Top ellipse (back dashed, front solid)
        \draw[tank, dashed, dash pattern=on 3pt off 2pt] (\R,0) arc (0:180:{\R} and {\E});
        \draw[tank] (-\R,0) arc (180:360:{\R} and {\E});

        % Sides
        \draw[tank] (-\R,0) -- (-\R,-\H);
        \draw[tank] (\R,0) -- (\R,-\H);

        % Bottom ellipse (back dashed, front solid)
        \draw[tank, dashed, dash pattern=on 3pt off 2pt] (-\R,-\H) arc (-180:0:{\R} and {\E});
        \draw[tank] (\R,-\H) arc (0:-180:{\R} and {\E});

        % --- water surface line (front solid, back dashed) ---
        \draw[blue!60!black, line width=1.1pt, dashed, dash pattern=on 3pt off 2pt]
        (\R,-\hw) arc (0:180:{\R} and {\E});
        \draw[blue!60!black, line width=1.1pt]
        (-\R,-\hw) arc (180:360:{\R} and {\E});

        % --- labels ---
        \node at (0,{-(\H+\hw)/2}) {$x_1(t)$};

        % --- inflow g_1(t) ---
        \draw[flow] (0,1.4) -- (0,0.35);
        \node[above] at (0,1.4) {$g_1(t)$};

        % --- outflow x2(t) ---
        \draw[flow] (\R,{-(\H+\hw)/2}) -- (\R+2.4,{-(\H+\hw)/2});
        \node[right] at (\R+2.4,{-(\H+\hw)/2}) {$x_2(t)$};

    \end{tikzpicture}
    \caption{Tank filling problem setup.}
    \label{fig:tank-filling}
\end{figure}

\subsubsection{Case A: Specified Outlet Flow (Index 1)}

Suppose we specify the outlet flow rate as $x_2 = g_2(t)$. The governing equations are the mass balance and the flow constraint:
\begin{align}
    (1) \quad & \dot{x}_1 + x_2 = g_1(t) \\
    (2) \quad & x_2 = g_2(t)
\end{align}
To find the index, we check how many derivatives are needed to define $\dot{x}_1$ and $\dot{x}_2$. We already have an expression for $\dot{x}_1$ from equation (1): $\dot{x}_1 = g_1(t) - x_2$. To get $\dot{x}_2$, we differentiate equation (2) once:
\begin{equation}
    \dot{x}_2 = \dot{g}_2(t)
\end{equation}
We have now found explicit ODEs for all states ($\dot{x}_1$ and $\dot{x}_2$) using only one differentiation step. Therefore, this is an index 1 DAE. The analytical solution involves an integral of the inputs:
\begin{equation}
    x_1(t) = x_1(t_0) + \int_{t_0}^t (g_1(t') - g_2(t')) dt', \qquad x_2(t) = g_2(t)
\end{equation}
Because integration is a smoothing operation, this system is well-behaved. Noise in the forcing functions $g_1$ or $g_2$ does not cause catastrophic errors in the solution.

\subsubsection{Case B: Specified Internal Volume (Index 2)}

Now consider a different control objective where we specify the internal volume trajectory $x_1 = g_2(t)$ directly. The equations are
\begin{align}
    \dot{x}_1 + x_2 &= g_1(t) \tag{1} \\
    x_1 &= g_2(t) \tag{2}
\end{align}
First, we address the algebraic constraint. Equation (2) defines $x_1$ directly, but Equation (1) depends on $\dot{x}_1$. To link these, we differentiate the constraint (2) with respect to time:
\begin{equation}
    \frac{d}{dt}(x_1) = \frac{d}{dt}(g_2(t)) \implies \dot{x}_1 = \dot{g}_2(t) \tag{2'}
\end{equation}
Next, we substitute this expression for $\dot{x}_1$ into Equation (1). This allows us to solve explicitly for the algebraic variable, $x_2$:
\begin{equation}
    \underbrace{\dot{g}_2(t)}_{\dot{x}_1} + x_2 = g_1(t) \implies x_2 = g_1(t) - \dot{g}_2(t)
\end{equation}
At this stage, we have expressions for $x_1$ and $x_2$, but we do not yet have a differential equation describing how $x_2$ changes over time. To find $\dot{x}_2$, we must differentiate the derived expression for $x_2$:
\begin{equation}
    \dot{x}_2 = \frac{d}{dt} \left( g_1(t) - \dot{g}_2(t) \right) = \dot{g}_1(t) - \ddot{g}_2(t)
\end{equation}
We have now successfully derived the complete set of first-order ODEs. We must count the number of differentiation steps required. We differentiated the original constraint ($x_1 = g_2(t)$) once to find $\dot{x}_1$ and a second time to find $\dot{x}_2$. Because two differentiations were required to transform the DAE into an explicit ODE system, this is classified as an index 2 system.

The analytical solution is
\begin{equation}
    x_1(t) = g_2(t), \qquad x_2(t) = g_1(t) - \dot{g}_2(t)
\end{equation}
Note the dependence on $\dot{g}_2(t)$. If the specified volume signal $g_2(t)$ is noisy, differentiation will amplify that noise. For example, if $g_2(t)$ has a small step change, its derivative contains an impulse (Dirac delta), causing violent transients in the calculated flow rate $x_2$. Index 2 (and higher) systems are essentially ``numerical differentiators,'' and lead to noise exaggeration.

% \begin{figure}[H]
%     \centering
%     \caption{Noise amplification in higher-index DAEs. For an index 2 system, a perturbation in the constraint leads to magnified noise in the algebraic states.}
%     \label{fig:index_noise}
% \end{figure}

This high index is often a result of poor modeling choices. Physically, we cannot force the volume to be exactly $g_2(t)$ directly. A more realistic model would implement a controller that adjusts the outlet flow $x_2$ based on the error between the actual volume $x_1$ and the setpoint $g_2$. Such a control implementation should result in a well-behaved index 1 system.

\subsection{Another Example: Calculation of Higher Indices}

We can apply this differentiation procedure to arbitrary systems to determine their index. Consider the following coupled system:
\begin{align}
    (1) \quad & \dot{x}_2 = x_1 \\
    (2) \quad & \dot{x}_3 = x_2 \\
    (3) \quad & x_3 = \gamma(t)
\end{align}
To find the index, we differentiate the algebraic constraint (3) repeatedly until we have expressions for $\dot{x}_1, \dot{x}_2, \text{and } \dot{x}_3$. Note that here $x_1$ is technically an algebraic variable (it lacks a derivative in the original equations), but we follow the convention of differentiating until $\dot{\mathbf{z}}$ is explicit for all components.

\textbf{Differentiation 1:} Differentiate (3) to get $\dot{x}_3$:
\begin{equation}
    (3') \quad \dot{x}_3 = \dot{\gamma}(t)
\end{equation}
Matching $(3')$ with (2), we find an algebraic relationship for $x_2$:
\begin{equation}
    x_2 = \dot{\gamma}(t)
\end{equation}

\textbf{Differentiation 2:} Differentiate the expression for $x_2$:
\begin{equation}
    (2') \quad \dot{x}_2 = \ddot{\gamma}(t)
\end{equation}
Matching $(2')$ with (1), we find an algebraic relationship for $x_1$:
\begin{equation}
    x_1 = \ddot{\gamma}(t)
\end{equation}

\textbf{Differentiation 3:} Differentiate the expression for $x_1$ to finally get $\dot{x}_1$:
\begin{equation}
    (1') \quad \dot{x}_1 = \dddot{\gamma}(t)
\end{equation}

Since we required 3 differentiations to obtain the differential equation for the final state $x_1$, this is an index 3 DAE. The complete set of underlying ODEs is:
\begin{align}
    \dot{x}_1 &= \dddot{\gamma}(t) \\
    \dot{x}_2 &= \ddot{\gamma}(t) \\
    \dot{x}_3 &= \dot{\gamma}(t)
\end{align}
As with the index 2 tank example, solving this system numerically is difficult because it effectively requires calculating the third derivative of the forcing function $\gamma(t)$, which is extremely sensitive to numerical error. In general, index 1 DAEs are much more amenable to numerical solution than systems with index $>1$.
\begin{warningBox}
\textbf{Warning:}
In this course, the differential index is the number of times we must differentiate the DAE system until we can write \emph{explicit first-order ODEs for every component of the full state} $\mathbf{z}=[\mathbf{x};\mathbf{y}]$. This usually means differentiating the algebraic constraints until we can write explicit ODEs for every component of $\mathbf{z}$. Here are some notes to keep in mind:

\begin{itemize}
    \item Do not equate ``\# algebraic equations'' with index. A system can have many algebraic constraints and still be index 1, or have a single constraint and be index 2+.
    \item Elimination can hide the index. If you substitute constraints to eliminate variables, you may get an ODE for $\mathbf{x}$ quickly, but under our definition you must still ask: how many differentiations are needed to determine $\dot{\mathbf{y}}$ (and hence $\dot{\mathbf{z}}$)?
    \item Watch for hidden consistency conditions. Differentiating constraints can introduce additional conditions that must hold at $t_0$ (e.g.\ involving $\dot{\gamma}(t_0)$, $\ddot{\gamma}(t_0)$). If initial conditions violate these, the model is not (classically) solvable without projection/re-initialization. We will see this in more detail later.
    \item Index is a structural property, not ``how hard it feels.''' Stiffness and fast/slow time scales affect numerics, but they are not the index. The index is about how many differentiations are required to make $\dot{\mathbf{z}}$ explicit.
\end{itemize}
\end{warningBox}

\section{Distinguishing Index 1 from Higher Index Systems}

We established earlier that index 1 DAEs are generally well-behaved and solvable with standard methods, while higher-index systems pose significant numerical challenges. But how do we determine the index of a given system without performing the repeated differentiation manually every time? We can do this by examining the Jacobian of the algebraic constraints. Let us revisit the standard semi-explicit form:
\begin{align}
    \dot{\mathbf{x}} &= \mathbf{f}(t, \mathbf{x}, \mathbf{y}) \\
    \mathbf{0} &= \mathbf{g}(t, \mathbf{x}, \mathbf{y})
\end{align}
Recall that $\mathbf{x}$ represents the differential states (those appearing with time derivatives) and $\mathbf{y}$ represents the algebraic states. To see if the system is index 1, we differentiate the algebraic equation $\mathbf{g} = \mathbf{0}$ with respect to time using the chain rule:
\begin{equation}
    \mathbf{0} = \frac{\partial \mathbf{g}}{\partial t} + \frac{\partial \mathbf{g}}{\partial \mathbf{x}}\dot{\mathbf{x}} + \frac{\partial \mathbf{g}}{\partial \mathbf{y}}\dot{\mathbf{y}}
\end{equation}
We can substitute the expression for $\dot{\mathbf{x}}$ from the first equation into this new relation:
\begin{equation}
    \mathbf{0} = \frac{\partial \mathbf{g}}{\partial t} + \frac{\partial \mathbf{g}}{\partial \mathbf{x}}\mathbf{f}(t, \mathbf{x}, \mathbf{y}) + \frac{\partial \mathbf{g}}{\partial \mathbf{y}}\dot{\mathbf{y}}
\end{equation}
Now, observe the term attached to $\dot{\mathbf{y}}$. If the Jacobian matrix $\frac{\partial \mathbf{g}}{\partial \mathbf{y}}$ is non-singular (invertible), we can rearrange this equation to solve explicitly for $\dot{\mathbf{y}}$:
\begin{equation}
    \dot{\mathbf{y}} = -\left( \frac{\partial \mathbf{g}}{\partial \mathbf{y}} \right)^{-1} \left( \frac{\partial \mathbf{g}}{\partial t} + \frac{\partial \mathbf{g}}{\partial \mathbf{x}}\mathbf{f}(t, \mathbf{x}, \mathbf{y}) \right)
\end{equation}
If this inversion is possible, we have successfully derived a complete set of ODEs for both $\mathbf{x}$ and $\mathbf{y}$ with just one differentiation. Therefore, the condition for a DAE to be index 1 is that the Jacobian of the constraints with respect to the algebraic variables, $\frac{\partial \mathbf{g}}{\partial \mathbf{y}}$, is non-singular.

\subsection{Example: Detecting Index > 1}

Let's apply this Jacobian test to the coupled system we analyzed previously:
\begin{align}
    (1) \quad \dot{x}_2 &= x_1 \\
    (2) \quad \dot{x}_3 &= x_2 \\
    (3) \quad x_3 &= \gamma(t)
\end{align}
First, we must classify the variables. The variables $x_2$ and $x_3$ appear with derivatives, so they are differential states. The variable $x_1$ appears only on the right-hand side of (1) and has no explicit derivative in the system; in a semi-explicit formulation $\dot{\mathbf{x}}=\mathbf{f}(\mathbf{x}, \mathbf{y})$, the variable $x_1$ plays the role of the algebraic state $\mathbf{y}$ because it defines the rate of change of a differential state but is not one itself. The algebraic equation is $0 = x_3 - \gamma(t)$.

Let us check the Jacobian of the constraint function $g = x_3 - \gamma(t)$ with respect to the algebraic state $x_1$:
\begin{equation}
    \frac{\partial g}{\partial x_1} = \frac{\partial}{\partial x_1}(x_3 - \gamma(t)) = 0
\end{equation}
The Jacobian is zero (a $1\times 1$ singular matrix). Because the Jacobian is singular, we cannot solve for $\dot{x}_1$ directly from the differentiated constraint. This confirms that the system is index > 1 (as we saw before, it is actually index 3).

% \section{Reducing the Index of a DAE}

% If we encounter a high-index problem, we are not necessarily stuck. In many regular DAEs encountered in modeling practice, we can apply an index-reduction procedure (often involving differentiating some constraints and/or introducing auxiliary variables) to obtain a formulation that is low index and therefore compatible with standard DAE integrators. This should not be interpreted as a guarantee that every DAE can be transformed in a globally equivalent way. If the model is structurally singular, inconsistent, or poorly posed, index reduction may fail or may introduce additional consistency conditions that must be enforced. 

% One common technique for index reduction is the method of auxiliary variables. To make this concrete, let's consider the index 2 tank filling problem where the volume $x_1$ is specified:
% \begin{align}
%     (1) \quad \dot{x}_1 + x_2 &= g_1(t) \\
%     (2) \quad x_1 &= g_2(t)
% \end{align}
% To reduce the index, we can introduce a new variable $x_3$ defined as the derivative of $x_1$:
% \begin{equation}
%     x_3 = \dot{x}_1
% \end{equation}
% We can now rewrite the system in terms of this new variable. We replace $\dot{x}_1$ in the first equation with $x_3$, and we add the derivative of the constraint equation:
% \begin{align}
%     (1) \quad x_3 + x_2 &= g_1(t) \\
%     (2) \quad x_1 &= g_2(t) \\
%     (3) \quad x_3 &= \dot{g}_2(t)
% \end{align}
% The resulting reformulation has three variables and three equations but, in this particular example, it is purely algebraic in time. Once $g_1(t)$, $g_2(t)$, and $\dot g_2(t)$ are known, we can evaluate $x_1(t)=g_2(t)$, $x_3(t)=\dot g_2(t)$, and then $x_2(t)=g_1(t)-x_3(t)$ without performing any time integration. In other words, this specific index-reduction step collapses the problem to an algebraic evaluation (often informally referred to as ``index $0$'' / trivially solvable), which is certainly low-index.

% While such reformulations can be very useful, a word of caution is warranted: drift arises when we replace an original constraint (e.g., $x_1=g_2(t)$) by only its differentiated form(s) and do not explicitly re-enforce the original constraint (for example, by projection or constraint stabilization). Above, we have retained $x_1=g_2(t)$ explicitly, so that particular constraint cannot drift at the discrete time points; drift becomes an issue in formulations that drop the original constraint and enforce only its time derivative(s).

\section{Reducing the Index of a DAE}

If we encounter a high-index problem, we are not necessarily stuck. In many/most regular DAEs encountered, we can apply an \textbf{index-reduction} procedure (usually involving differentiating some constraints and/or introducing auxiliary variables) to obtain a formulation that is low index and therefore compatible with standard DAE integrators. This should not be interpreted as a guarantee that every DAE can be transformed in a globally equivalent way. If the model is structurally singular, inconsistent, or poorly posed, index reduction may fail or may introduce additional consistency conditions that must be enforced.

\paragraph*{Why do we care?}
Most general-purpose DAE solvers are designed for index-1 systems. When given an index $>1$ formulation, they likely either (i) fail during initialization/Newton solves, (ii) exhibit severe order reduction, or (iii) attempt internal symbolic manipulations (e.g., differentiating constraints) to reduce the index. A practical way to recognize index $>1$ behavior is that the solution (or consistent initialization) depends on time derivatives of prescribed signals and/or on derivatives of algebraic constraints.

\subsection{Auxiliary-Variable Index Reduction}

A common index-reduction technique is the \textbf{method of auxiliary variables}. The idea is simple:

\begin{itemize}
    \item Identify a quantity that appears as a \emph{derivative} (e.g., $\dot x_1$) but is \emph{not} directly represented as an independent variable in the model.
    \item Introduce a new variable to represent that derivative (e.g., $x_2 := \dot x_1$).
    \item Replace every occurrence of the derivative by the auxiliary variable.
    \item Add additional equations obtained by differentiating the appropriate algebraic constraint(s) until the enlarged system becomes index 1.
\end{itemize}

This procedure trades hidden derivative constraints (which cause high index) for explicit algebraic equations involving the new auxiliary variables. The resulting system may have more variables and more equations, but it should be much easier to initialize and integrate numerically.

As an example, let's consider the system
\begin{align}
    (1)\quad & \dot x_1 = y_1 - y_2 \\
    (2)\quad & x_1 = e^{-t} \\
    (3)\quad & y_1 + y_2 = 1
\end{align}
Here, $x_1$ is constrained algebraically by (2), while (1) requires $\dot x_1$; this is a common signature of an index-$>1$ formulation. We differentiate the algebraic constraints once:
\begin{align}
    (2')\quad & \dot x_1 = -e^{-t} \\
    (3')\quad & \dot y_1 + \dot y_2 = 0
\end{align}
Equation (2') is an implicit (hidden) constraint on the dynamics. Since $x_1=e^{-t}$ must hold for all $t$, its derivative must also hold, so $\dot x_1$ is not free. Combining (1) and (2') already forces
\begin{equation}
y_1 - y_2 = -e^{-t}
\end{equation}
and further differentiations are required to obtain explicit ODEs for all variables. Indeed, this is an index-2 system. Now, we define an auxiliary variable
\begin{equation}
    x_2 := \dot x_1
\end{equation}
and replace $\dot x_1$ in (1) by $x_2$. Then, we make the hidden constraint explicit by including (2') as an algebraic equation:
\begin{align}
    (1)\quad & x_2 = y_1 - y_2 \\
    (2)\quad & x_1 = e^{-t} \\
    (3)\quad & y_1 + y_2 = 1 \\
    (4)\quad & x_2 = -e^{-t}
\end{align}
This enlarged system is index 1. The constraints now determine $(x_1,x_2,y_1,y_2)$ directly at each time without requiring further differentiation.

Foreshadowing a later section (don't worry if this doesn't make sense right now), in this example, there are no degrees of freedom. At $t=t_0$, equation (2) fixes $x_1(t_0)=e^{-t_0}$, equation (4) fixes $x_2(t_0)=-e^{-t_0}$, and equations (1) and (3) then uniquely determine $(y_1(t_0),y_2(t_0))$:
\begin{equation}
y_1(t_0)=\frac{1+x_2(t_0)}{2},\qquad
y_2(t_0)=\frac{1-x_2(t_0)}{2}
\end{equation}
So consistent initialization leaves $0$ DoF in this particular case.

\paragraph*{Connecting Back to the Tank Example}
Let us revisit the index-2 tank filling problem where the volume is specified:
\begin{align}
    (1)\quad & \dot{x}_1 + x_2 = g_1(t) \\
    (2)\quad & x_1 = g_2(t)
\end{align}
Introducing an auxiliary variable for the derivative,
\begin{equation}
x_3 := \dot x_1
\end{equation}
and making the differentiated constraint explicit gives
\begin{align}
    (1)\quad & x_3 + x_2 = g_1(t) \\
    (2)\quad & x_1 = g_2(t) \\
    (3)\quad & x_3 = \dot g_2(t)
\end{align}
Again, we have reduced the index from 2 to 1 by introducing an auxiliary variable.

\paragraph*{Caution: drift and consistency.}
Index reduction can introduce differentiated constraints such as $x_3=\dot g_2(t)$ (or, more generally, relations obtained by differentiating $\mathbf g(t,\mathbf x,\mathbf y)=\mathbf 0$). Two practical warnings: (1) Differentiated constraints impose extra requirements at $t_0$ (e.g., involving $\dot g_2(t_0)$, $\ddot g_2(t_0)$, etc.). If these are violated, the original model is not classically solvable without re-initialization/projection. (2) If we replace an original constraint by only its differentiated form(s) and do not re-enforce the original constraint (via projection or stabilization), numerical solutions may drift off the true constraint manifold. In the reformulations above, we retained the original constraints (e.g.\ $x_1=g_2(t)$), so those constraints cannot drift at the discrete solution points.


\section{Numerical Simulation of DAEs}

We approach solving DAEs numerically in the same way as ODE-IVPs. We discretize time and estimate the solution at a finite set of times. However, the presence of algebraic constraints introduces stiffness issues.

\subsection{Connection to Stiffness}

We can think of a DAE as the limit of a very stiff ODE. Consider the system:
\begin{align}
    \dot{\mathbf{x}} &= \mathbf{f}(t, \mathbf{x}, \mathbf{y}) \\
    \epsilon \dot{\mathbf{y}} &= \mathbf{g}(t, \mathbf{x}, \mathbf{y})
\end{align}
If we take the limit as $\epsilon \to 0$, we recover the DAE $\mathbf{0} = \mathbf{g}(t, \mathbf{x}, \mathbf{y})$. For a very small but non-zero $\epsilon$, the term $\dot{\mathbf{y}}$ would be very large, corresponding to extremely fast dynamics that must be resolved on arbitrarily short time scales. This is the definition of a stiff problem. Qualitatively, we can interpret a DAE as an ``infinitely stiff'' ODE. Consequently, explicit methods (like forward Euler) generally fail, and we must use implicit methods (stiff solvers).

\subsection{Failure of Forward Euler}

If we try to use the forward Euler method on a semi-explicit DAE $\dot{\mathbf{x}} = \mathbf{f}(t, \mathbf{x}, \mathbf{y})$, we calculate the next step for the differential states as
\begin{equation}
    \mathbf{x}(t_{i+1}) = \mathbf{x}(t_i) + \Delta t \, \mathbf{f}(t_i, \mathbf{x}(t_i), \mathbf{y}(t_i))
\end{equation}
However, simply updating $\mathbf{x}$ does not guarantee that the new state $\mathbf{x}(t_{i+1})$ satisfies the algebraic constraint $\mathbf{0} = \mathbf{g}(t_{i+1}, \mathbf{x}(t_{i+1}), \mathbf{y}(t_{i+1}))$. The solution will drift off the constraint manifold. 

Instead, the key issue is that an explicit update of $\mathbf{x}$ alone does not enforce the constraint manifold. A simple remedy (for index-1 systems) is an explicit--projection step: first compute $\mathbf{x}(t_{i+1})$ explicitly using forward Euler, and then determine $\mathbf{y}(t_{i+1})$ by solving the algebraic constraint at the new time,
\begin{align}
\mathbf{x}(t_{i+1}) &= \mathbf{x}(t_i) + \Delta t\,\mathbf{f}(t_i,\mathbf{x}(t_i),\mathbf{y}(t_i)),\\
\mathbf{0} &= \mathbf{g}(t_{i+1},\mathbf{x}(t_{i+1}),\mathbf{y}(t_{i+1})).
\end{align}
This generally requires a (possibly nonlinear) solve for $\mathbf{y}(t_{i+1})$ only (assuming $\partial \mathbf{g}/\partial \mathbf{y}$ is nonsingular). Fully implicit methods go further by making $\mathbf{x}(t_{i+1})$ depend on $\mathbf{y}(t_{i+1})$ as well; in that case the step typically involves a coupled nonlinear solve for both sets of variables.

\subsection{Backward Euler and Simulation Errors}

To handle this coupling robustly, we typically use fully implicit methods like backward Euler. For a general DAE $\mathbf{f}(t, \mathbf{z}, \dot{\mathbf{z}}) = \mathbf{0}$, we approximate the derivative with a backward difference:
\begin{equation}
    \mathbf{f}\left( t_{i+1}, \mathbf{z}(t_{i+1}), \frac{\mathbf{z}(t_{i+1}) - \mathbf{z}(t_i)}{\Delta t} \right) \approx \mathbf{0}
\end{equation}
At each time step, this requires solving a system of nonlinear equations for $\mathbf{z}(t_{i+1})$. Generalized multistep methods, such as those used in MATLAB's \texttt{ode15s}, \texttt{ode23t}, and \texttt{ode15i}, are built on this principle.

However, for higher-index systems, standard low-order methods can result in surprisingly poor accuracy. Let's return to our index 2 stirred tank system (specified output) to analyze the error scaling. The equations are
\begin{align}
    \frac{dc_2}{dt} &= \frac{Q}{V}(c_1(t) - c_2(t)) \\
    c_2(t) &= \gamma(t)
\end{align}
Using backward Euler, the discrete step for $c_2$ is simply the constraint $c_2(t_{i+1}) = \gamma(t_{i+1})$. We substitute the backward difference approximation for the derivative into the first equation to solve for the input $c_1$:
\begin{equation}
    c_1(t_{i+1}) = c_2(t_{i+1}) + \frac{V}{Q} \left( \frac{c_2(t_{i+1}) - c_2(t_i)}{\Delta t} \right) + O(\Delta t)
\end{equation}
The term in parentheses is a first-order backward-difference approximation of $\dot c_2(t_{i+1})$. If $c_2$ were known \emph{exactly} at the grid points (e.g., if we can evaluate $\gamma(t_i)$ exactly and enforce $c_2(t_i)=\gamma(t_i)$ to solver tolerance), then this derivative approximation incurs an $O(\Delta t)$ truncation error, and the induced error in the algebraically computed input $c_1(t_{i+1})$ is correspondingly $O(\Delta t)$.

However, the situation becomes much worse in the typical index-2 setting where the constrained quantity is only satisfied \emph{approximately} at each step. Suppose the numerical values satisfy $\tilde c_2(t_i)=c_2(t_i)+e_i$ with $e_i=O(\Delta t)$ (for example due to nonlinear-solve tolerances or accumulated discretization effects). Then the difference quotient contains the amplified term
\[
\frac{\tilde c_2(t_{i+1})-\tilde c_2(t_i)}{\Delta t}
=
\frac{c_2(t_{i+1})-c_2(t_i)}{\Delta t} + \frac{e_{i+1}-e_i}{\Delta t},
\]
and $\frac{e_{i+1}-e_i}{\Delta t}=O(1)$ in general. Because $c_1$ depends \emph{directly} on this quotient, the error in the computed $c_1$ can therefore be $O(1)$ unless constraints are enforced very accurately and/or a higher-order discretization is used. This is a concrete manifestation of the well-known \emph{order reduction} that can occur for higher-index DAEs, especially in variables that behave like time derivatives of constrained quantities.
\begin{equation}
    c_2(t_{i+1}) = c_2(t_i) + \frac{\Delta t}{2} \left[ \left.\frac{dc_2}{dt}\right|_{t_i} + \left.\frac{dc_2}{dt}\right|_{t_{i+1}} \right] + O(\Delta t^3)
\end{equation}
Solving for $c_1(t_{i+1})$ under this scheme, we get an expression where the error terms scale much more favorably:
\begin{equation}
    c_1(t_{i+1}) = c_2(t_{i+1}) - c_1(t_i) + c_2(t_i) + \frac{2V}{Q\Delta t}(c_2(t_{i+1}) - c_2(t_i)) + O(\Delta t^2)
\end{equation}
With the trapezoidal update, the GTE scales as $O(\Delta t)$, which is much more reasonable and helps ensure convergence as the step size decreases.

\section{Initialization of Differential-Algebraic Equations}

A challenge that distinguishes DAEs from ODEs is the problem of initialization. For a standard ODE system, finding a valid starting point is usually a matter of choice; for a DAE, it is a matter of consistency. Consider a generic system written in fully implicit form:
\begin{equation}
    \mathbf{f}(\dot{\mathbf{x}}(t), \mathbf{x}(t), t) = \mathbf{0}
\end{equation}
To start a numerical integrator at time $t_0$, we theoretically need to determine the values of both the state vector $\mathbf{x}(t_0)$ and its time derivative $\dot{\mathbf{x}}(t_0)$. If the system consists of $n$ equations with $n$ variables, there are a total of $2n$ unknowns involved in the initialization: the set $\{\mathbf{x}(t_0), \dot{\mathbf{x}}(t_0)\}$. Since we have $n$ equations, we nominally have $n$ degrees of freedom remaining.

For an explicit ODE system where $\dot{\mathbf{x}} = \mathbf{h}(\mathbf{x}, t)$, this is straightforward. We arbitrarily choose/specify the initial state $\mathbf{x}(t_0)$, and the equations themselves immediately determine the derivative $\dot{\mathbf{x}}(t_0)$. However, DAEs include algebraic constraints that restrict the manifold on which the solution can exist. We cannot simply choose an arbitrary $\mathbf{x}(t_0)$ because it might violate the algebraic equations. Furthermore, DAEs often contain hidden or implicit constraints that restrict the derivatives as well. Generally, solving $\mathbf{f}=\mathbf{0}$ at $t_0$ provides $n$ constraints, so we must be careful to identify which of the $2n$ values can be freely specified.

\subsection{Consistent Initialization and Implicit Constraints}

To illustrate the complexity of consistent initialization, let us revisit the stirred tank reactor system with a variable volume, which we previously identified as an index 2 DAE. The governing equations are
\begin{equation}
    \begin{aligned}
        \frac{dc_2}{dt} &= \frac{Q}{V}(c_1(t) - c_2(t)) \\
        c_2(t) &= \gamma(t)
    \end{aligned}
\end{equation}
We assume $\gamma(t)$ is sufficiently smooth near $t_0$ so that the required derivatives (e.g., $\dot\gamma(t_0)$) exist. At first glance, we might expect that only $c_2(t_0)$ needs to be specified or perhaps determined by the algebraic equation, while $c_1(t_0)$ is a free input variable. However, if we examine the discretized update equation (which a numerical solver would use to march forward), we see a dependency on the previous time step:
\begin{equation}
    c_1(t_{i+1}) = c_2(t_{i+1}) - c_1(t_i) + c_2(t_i) + \frac{2V}{Q\Delta t}(c_2(t_{i+1}) - c_2(t_i)) + O(\Delta t^2)
\end{equation}
To implement this numerical solution, we clearly need specific values for both $c_1(t_0)$ and $c_2(t_0)$ to initialize the loop. We cannot set these arbitrarily. We must obey the governing equations.

The algebraic equation explicitly fixes the second concentration:
\begin{equation}
    c_2(t_0) = \gamma(t_0)
\end{equation}
However, because this constraint holds for all time ($c_2(t) = \gamma(t)$), its time derivative must also hold ($\dot{c}_2(t) = \dot{\gamma}(t)$). Substituting this hidden constraint into the differential equation reveals a constraint on $c_1$:
\begin{equation}
    \frac{dc_2}{dt} = \frac{d\gamma}{dt} = \frac{Q}{V}(c_1 - c_2) \implies c_1 = c_2 + \frac{V}{Q}\dot{\gamma} = \gamma + \frac{V}{Q}\dot{\gamma}
\end{equation}
Thus, $c_1(t_0)$ is not a degree of freedom; it is completely determined by the forcing function $\gamma(t)$ and its derivative. In this case, we actually have zero degrees of freedom. This aligns with the general heuristic that an index $k$ DAE implies at least $k-1$ implicit constraints.

Let us apply this logic to two practice examples to determine the degrees of freedom available for initialization.

\paragraph*{Index 1 DAE Example:} Consider the system:
\begin{equation}
    \begin{aligned}
        \dot{x}_1 + x_2 &= g_1(t) \\
        x_2 &= g_2(t)
    \end{aligned}
\end{equation}
We need to specify the set $\{\dot{x}_1(t_0), x_1(t_0), x_2(t_0)\}$, totaling 3 values. We have 2 obvious constraints from the original equations at $t_0$:
\begin{equation}
    \dot{x}_1(t_0) + x_2(t_0) = g_1(t_0) \quad \text{and} \quad x_2(t_0) = g_2(t_0)
\end{equation}
This leaves 1 degree of freedom (DoF). We must specify $x_1(t_0)$. Note that we can check for implicit constraints by differentiating the algebraic equation: $\dot{x}_2(t_0) = \dot{g}_2(t_0)$. However, neither of the original equations relies on $\dot{x}_2$, so this does not constrain our choices for $x_1$ or $\dot{x}_1$. Thus, $x_1(t_0)$ is the only value we can freely choose.

\paragraph*{Index 3 DAE Example:} Now consider the familiar Index 3 system:
\begin{equation}
    \begin{aligned}
        (1) \quad \dot{x}_2 &= x_1 \\
        (2) \quad \dot{x}_3 &= x_2 \\
        (3) \quad x_3 &= \gamma(t)
    \end{aligned}
\end{equation}
Here, we must identify values for $\{\dot{x}_2, \dot{x}_3, x_2, x_3, x_1\}$ at $t_0$. To find the consistent initialization, we look at the derivatives generated while reducing the index. The algebraic constraint (3) immediately gives $x_3(t_0) = \gamma(t_0)$. Differentiating this once gives an implicit constraint on $x_2$:
\begin{equation}
    \dot{x}_3 = \dot{\gamma}(t) \implies x_2(t_0) = \dot{\gamma}(t_0)
\end{equation}
Differentiating again gives an implicit constraint on $x_1$:
\begin{equation}
    \dot{x}_2 = \ddot{\gamma}(t) \implies x_1(t_0) = \ddot{\gamma}(t_0)
\end{equation}
Consequently, the values of $x_3(t_0)$, $x_2(t_0)$, and $x_1(t_0)$ are all fixed by the external forcing function $\gamma(t)$ and its derivatives. There are no degrees of freedom left for the user to specify, despite the appearance of multiple variables. We possess at least two implicit constraints corresponding to the repeated differentiation required to reduce the index.

\paragraph*{Aside: Unusual Constraint Structures}

While the heuristic ``index $k$ has $k-1$ implicit constraints'' is fairly reliable, we should always check the specific system. It is possible (though uncommon) for an index 1 system to possess implicit constraints if the equations are structurally singular. For example, let's look at the system
\begin{equation}
    \begin{aligned}
        \dot{x}_1 + 2\dot{x}_2 &= x_1 - x_2 \\
        \dot{x}_1 + 2\dot{x}_2 &= 3x_1 - 2x_2 - 1
    \end{aligned}
\end{equation}
Subtracting the first equation from the second eliminates the derivatives entirely to uncover an implied algebraic constraint:
\begin{equation}
    0 = 2x_1 - x_2 - 1 \implies 0 = 2\dot{x}_1 - \dot{x}_2
\end{equation}
This system behaves as a DAE despite appearing to be a system of implicit ODEs.

\paragraph*{Practical Comments on Solvers}

When using numerical software, consistent initialization is non-negotiable. For example, MATLAB's \texttt{ode15i} (for fully implicit problems of the form $F(t,y,y')=0$) requires the user to provide both $y(t_0)$ and $y'(t_0)$, and these must satisfy $F(t_0,y(t_0),y'(t_0))=0$. If the supplied values are inconsistent, the nonlinear solve at the initial step can fail (e.g., due to lack of convergence or step-size reduction). MATLAB therefore provides \texttt{decic} to compute consistent initial conditions close to user-provided guesses; \texttt{decic} can itself fail to converge, in which case consistent values must be determined by additional analysis of the model.

\emph{(Please see slides for the index 2 example showing some of these methods in practice; uploading this book without that example for now.)}

\section{Exercises}

\begin{exercise}
Placeholder - work in progress 2
\end{exercise}
\begin{solution}
Placeholder - work in progress 2
\end{solution}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End: