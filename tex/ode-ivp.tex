\section{Numerical Solution of Initial Value Problems}
\label{sec:ode-ivp}

Many, if not most, of the physical processes encountered in chemical engineering are dynamic. First-principles models describing these processes often take the form of differential equations. This chapter focuses on a particularly common class of these problems: the Ordinary Differential Equation Initial Value Problem (ODE-IVP).

\subsection{Ordinary Differential Equation Initial Value Problems}
\label{subsec:ivp-intro}

An ODE-IVP describes the evolution of a system over time, starting from a known initial state. Mathematically, we can represent a general first-order ODE-IVP as follows:
\begin{align}
    \frac{d\vec{x}(t)}{dt} &= \vec{f}(\vec{x}(t), \vec{u}(t), t; \vec{\theta}) \\
    \vec{x}(t_0) &= \vec{x}_0
\end{align}
Here, $\vec{x}(t)$ is the \textbf{state vector}, which contains the set of dynamic variables we aim to solve for. The vector $\vec{f}$ represents the system's dynamics, which can be a function of the state $\vec{x}(t)$, time-dependent inputs $\vec{u}(t)$, time $t$ itself, and a vector of time-independent parameters $\vec{\theta}$. The problem is defined by the \textbf{initial condition} $\vec{x}_0$ at a starting time $t_0$. Our goal is typically to find the value of the state vector $\vec{x}(t)$ over a time domain $t \in [t_0, t_f]$. For simplicity, we often set the initial time $t_0 = 0$ and absorb the inputs and parameters into the function $\vec{f}$, leading to a more compact representation:
\begin{align}
    \frac{d\vec{x}}{dt} &= \vec{f}(\vec{x}, t) \\
    \vec{x}(t_0) &= \vec{x}_0
\end{align}
Since $\vec{f}(\vec{x}, t)$ can be an arbitrarily complex and nonlinear function, a closed-form analytical solution is rarely available. Therefore, we turn to numerical methods, which find the state vector at a finite number of points within the time domain. When using these methods, we must always consider their accuracy and stability, as these properties determine the reliability of our solutions.

A crucial condition for a well-posed ODE-IVP is that a unique solution exists. This is guaranteed if the function $\vec{f}(\vec{x},t)$ is \textbf{Lipschitz continuous} within the domain of interest. A function $\vec{f}$ is Lipschitz continuous if there exists a positive constant $m$ such that for any two points $\vec{x}$ and $\vec{z}$ in the domain, the following inequality holds:
\begin{equation}
    ||\vec{f}(\vec{x},t) - \vec{f}(\vec{z},t)||_p \le m||\vec{x}-\vec{z}||_p
    \label{eq:lipschitz}
\end{equation}
This is a stronger condition than regular continuity. For example, the function $f(x)=x^2$ is continuous everywhere but is not globally Lipschitz continuous because its slope can be arbitrarily large. By contrast, $f(x)=x$ is Lipschitz on $\mathbb{R}$; for instance, $|f(x)-f(z)|\le |x-z|$ shows a Lipschitz constant $m=1$ in the usual norm. The same conclusion holds for vector-valued identity mappings: with $\vec{f}(\vec{x})=\vec{x}$ and any $p$-norm, $\|\vec{f}(\vec{x})-\vec{f}(\vec{z})\|_p=\|\vec{x}-\vec{z}\|_p$, so a global Lipschitz constant $m=1$ applies. If $\vec{f}$ is not Lipschitz continuous, a solution might not exist for all time or might not be unique.

\subsubsection{Existence and uniqueness: illustrative examples}
The scalar problem $\dot{x} = x^2$ with $x(0)=x_0$ admits a separated solution. Integrating $\tfrac{dx}{x^2}=dt$ yields $\tfrac{1}{x_0}-\tfrac{1}{x(t)}=t$, hence
\begin{equation}
    x(t) = \frac{1}{\tfrac{1}{x_0}-t},
\end{equation}
which blows up in finite time at $t=\tfrac{1}{x_0}$ when $x_0>0$. Thus a solution may fail to exist for all times.

The problem $t\,\dot{x}=x-1$ with $x(0)=1$ can be separated as $\tfrac{dx}{x-1}=\tfrac{dt}{t}$, giving $\log(x-1)=\log t + c$ and hence $x(t)=1+c_2 t$. At $t=0$ the right-hand side is non-Lipschitz, and multiple solutions pass through the initial condition, demonstrating non-uniqueness.

\subsection{Conversion of Higher-Order ODEs}
\label{subsec:ivp-higher-order}
Many models in science and engineering involve second-order or higher-order differential equations. Fortunately, any higher-order ODE can be rewritten as a system of first-order ODEs, which is the standard form required by numerical solvers.

\begin{exampleBox}
    \textbf{Example}: Consider the force balance on a driven mass-spring-damper system, a classic second-order ODE:
    \begin{equation*}
        m\frac{d^2x}{dt^2} + b\frac{dx}{dt} + kx = F(t)
    \end{equation*}
    To convert this into a first-order system, we define a new state variable for the velocity, $v = \frac{dx}{dt}$. This immediately gives us our first equation:
    \begin{equation*}
        \frac{dx}{dt} = v
    \end{equation*}
    Next, we substitute $v$ into the original equation and solve for $\frac{dv}{dt}$:
    \begin{align*}
        m\frac{dv}{dt} + bv + kx &= F(t) \\
        \frac{dv}{dt} &= \frac{F(t) - bv - kx}{m}
    \end{align*}
    We can now define a state vector $\vec{x}(t) = \begin{pmatrix} x(t) \\ v(t) \end{pmatrix}$ and write the complete first-order system:
    \begin{equation*}
        \frac{d}{dt}\begin{pmatrix} x(t) \\ v(t) \end{pmatrix} = \begin{pmatrix} v(t) \\ \frac{F(t) - bv(t) - kx(t)}{m} \end{pmatrix}
    \end{equation*}
    The initial condition for this system would require specifying both the initial position and initial velocity, $\vec{x}(t_0) = \begin{pmatrix} x(t_0) \\ v(t_0) \end{pmatrix}$.
\end{exampleBox}

\subsection{Explicit Methods}
\label{subsec:ivp-explicit}
Numerical methods for ODE-IVPs operate by taking discrete steps in time. \textbf{Explicit methods} are a class of techniques that approximate the solution at a future time, $t_{i+1}$, using only known information at the current time, $t_i$. The general form for an explicit method is:
\begin{equation}
    \vec{x}(t_{i+1}) = \vec{x}(t_{i}) + \Delta t_i \vec{g}(\vec{x}(t_i), t_i, \Delta t_i)
    \label{eq:explicit_general}
\end{equation}
Here, $\Delta t_i = t_{i+1} - t_i$ is the step size, and the function $\vec{g}$ is designed to approximate the true rate of change of $\vec{x}$ over the interval. The accuracy of this approximation is quantified by the local truncation error.

\subsection{Error Analysis}
\label{subsec:ivp-error-analysis}
To understand the accuracy of a numerical method, we analyze its error. The \textbf{local truncation error (LTE)} is the error introduced in a single step, assuming the solution at the start of the step, $\vec{x}(t_i)$, is exact. We can quantify it by comparing the numerical solution $\vec{x}(t_{i+1})$ with a Taylor series expansion of the exact solution, $\vec{x}_{\text{exact}}(t_{i+1})$, around $t_i$:
\begin{equation}
    \vec{x}_{\text{exact}}(t_{i+1}) = \vec{x}(t_i) + \Delta t_i \frac{d\vec{x}}{dt}\bigg|_{t_i} + \frac{\Delta t_i^2}{2!} \frac{d^2\vec{x}}{dt^2}\bigg|_{t_i} + \dots = \vec{x}(t_i) + \sum_{j=1}^{\infty} \frac{\Delta t_i^j}{j!} \frac{d^j\vec{x}}{dt^j}\bigg|_{t_i}
    \label{eq:taylor_expansion}
\end{equation}
A method is said to have a local truncation error of order $p$, or $O(\Delta t_i^p)$, if the magnitude of the error scales with $\Delta t_i^p$ as the step size approaches zero. In contrast, the \textbf{global truncation error} is the accumulated error at the end of the entire integration interval $[t_0, t_f]$. If a method has an LTE of $O(\Delta t^{p+1})$, its global error is typically $O(\Delta t^p)$, as the local errors accumulate over approximately $N \sim 1/\Delta t$ steps.

It is often convenient to define the LTE for a single step via a vector norm. Let $\varepsilon_i = \big\|\vec{x}_{\text{exact}}(t_{i+1}) - \vec{x}(t_{i+1})\big\|$, where the norm may be any convenient choice. We say the LTE is $O(\Delta t_i^{p})$ if the limit $\lim_{\Delta t_i\to 0^+} \varepsilon_i/\Delta t_i^p = C$ exists with $C>0$.

\subsection{Forward Euler Method}
\label{subsec:ivp-forward-euler}
The simplest explicit method is the \textbf{Forward Euler} (or explicit Euler) method. It is derived by truncating the Taylor series after the first-order term or, equivalently, by using a forward finite difference approximation for the derivative. The update formula is:
\begin{equation}
    \vec{x}(t_{i+1}) = \vec{x}(t_i) + \Delta t_i \vec{f}(\vec{x}(t_i), t_i)
    \label{eq:forward_euler}
\end{equation}
By comparing this to the Taylor series expansion (\autoref{eq:taylor_expansion}), we see that the Forward Euler method neglects terms of order $\Delta t_i^2$ and higher. Therefore, its local truncation error is $O(\Delta t_i^2)$, and its global error is $O(\Delta t_i)$. Equivalently, as $\Delta t_i\to 0$, the ratio $\varepsilon_i/\Delta t_i^2$ tends to a positive constant. The exact solution satisfies the integral identity
\begin{equation}
    \vec{x}(t_{i+1}) = \vec{x}(t_i) + \int_{t_i}^{t_{i+1}} \vec{f}(\vec{x}(t'), t')\,dt',
\end{equation}
and Forward Euler replaces this integral by $\Delta t_i\,\vec{f}(\vec{x}(t_i), t_i)$, which is analogous to using a left Riemann sum for numerical integration.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/ode_ivp/forward_euler_demo.pdf}
    \caption{Visualization of the Forward Euler method on $x'=-x$, $x(0)=1$. Each step advances along the tangent at the left endpoint, illustrating the left Riemann-sum analogy.}
    \label{fig:forward-euler-viz}
\end{figure}

\subsubsection{Global error of Forward Euler}
A direct derivation on the scalar test problem clarifies the $O(\Delta t)$ global accuracy. The first step gives $x(t_1)=x(t_0)+\Delta t\,f(x(t_0),t_0)=x_{\text{exact}}(t_1)+O(\Delta t^2)$. The next step yields $x(t_2)=x(t_1)+\Delta t\,f(x(t_1),t_1)=x_{\text{exact}}(t_1)+O(\Delta t^2)+\Delta t\,f(x_{\text{exact}}(t_1)+O(\Delta t^2),t_1)=x_{\text{exact}}(t_2)+O(2\,\Delta t^2)$. After $N$ steps, $x(t_{N+1})=x_{\text{exact}}(t_N)+O(N\,\Delta t^2)$. Since $N\sim 1/\Delta t$ over a fixed interval, the accumulated error is $O(\Delta t)$.

\subsubsection{Numerical Stability}
\label{subsec:ivp-stability}
Accuracy is not the only concern; a numerical method must also be \textbf{stable}. An unstable method can produce solutions that grow without bound, even when the true solution is bounded. To analyze stability, we use a simple linear prototype problem:
\begin{equation}
    \frac{dx}{dt} = \lambda x, \quad x(0) = 1
\end{equation}
where $\lambda$ can be a complex number. The exact solution is $x(t) = e^{\lambda t}$. This solution is stable (decays to zero) if $\text{Re}(\lambda) \le 0$. For a numerical method to be stable, its solution must also decay to zero under the same condition.

Applying Forward Euler to the prototype problem gives:
\begin{equation}
    x(t_{i+1}) = x(t_i) + \Delta t (\lambda x(t_i)) = (1 + \lambda \Delta t) x(t_i)
\end{equation}
After $i+1$ steps, the solution is $x(t_{i+1}) = (1 + \lambda \Delta t)^{i+1} x(t_0)$. The numerical solution decays if the amplification factor, $|1 + \lambda \Delta t|$, is less than or equal to 1. For a linear system $\dot{\vec{x}}=A\,\vec{x}$ with a complete eigenbasis $A=W^{-1}\Lambda W$ and modal coordinates $\vec{y}=W\,\vec{x}$, each mode updates by $y_j(t_{i+1})=(1+\lambda_j\Delta t)\,y_j(t_i)$. The squared modulus satisfies
\begin{equation}
    |1+\lambda_j\Delta t|^2 = (1+\mathrm{Re}\,\lambda_j\,\Delta t)^2 + (\mathrm{Im}\,\lambda_j\,\Delta t)^2 \le 1.
\end{equation}
For a system of ODEs, this condition must hold for every eigenvalue $\lambda_j$ of the system's Jacobian matrix $A$. This requirement defines a \textbf{stability region} in the complex plane for the value $\lambda \Delta t$. For Forward Euler, this region is a circle of radius 1 centered at $(-1, 0)$. On the negative real axis with $\lambda<0$, it is convenient to define the characteristic time scale $\tau=-1/\lambda$; the scalar stability requirement reduces to $0\le \Delta t \le 2\tau$.
\begin{equation}
    |1 + \lambda_j \Delta t| \le 1
\end{equation}
This means Forward Euler is only \textbf{conditionally stable}. If the system has a large negative real eigenvalue (a very fast, stable dynamic), the step size $\Delta t$ must be made extremely small to keep $\lambda \Delta t$ within the stability region.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/ode_ivp/forward_euler_stability_region.pdf}
    \caption{Stability region for Forward Euler in the $z=\lambda\Delta t$ plane: $|1+z|\le 1$ (unit circle centered at $-1$).}
    \label{fig:forward-euler-stability}
\end{figure}

\subsubsection{Higher-Order Explicit Methods}
\label{subsec:ivp-higher-order-explicit}
The first-order accuracy of the Forward Euler method is often insufficient. Higher-order methods can achieve better accuracy with larger step sizes.

The \textbf{Midpoint Method} is a two-stage method that offers second-order accuracy. It first takes a half-step using Forward Euler to estimate the state at the midpoint of the interval, then uses the slope at that midpoint to take the full step.
\begin{align}
    \vec{y} &= \vec{x}(t_i) + \frac{\Delta t_i}{2} \vec{f}(\vec{x}(t_i), t_i) \\
    \vec{x}(t_{i+1}) &= \vec{x}(t_i) + \Delta t_i \vec{f}(\vec{y}, t_i + \Delta t_i / 2)
\end{align}
The local truncation error of the Midpoint method is $O(\Delta t_i^3)$, leading to a global error of $O(\Delta t_i^2)$. A short derivation follows by writing $\vec{g}(\vec{x}(t_i),t_i,\Delta t_i)=\vec{f}(\vec{x}(t_i)+\tfrac{\Delta t_i}{2}\vec{f}(\vec{x}(t_i),t_i),\,t_i+\tfrac{\Delta t_i}{2})$ and expanding at $\Delta t_i=0$ as
\begin{equation}
    \vec{g}(\vec{x}(t_i),t_i,\Delta t_i) = \vec{f}(\vec{x}(t_i),t_i) + \frac{\Delta t_i}{2}\,\frac{d\vec{f}}{dt}\bigg|_{(\vec{x}(t_i),t_i)} + O(\Delta t_i^2) = \vec{f}(\vec{x}(t_i),t_i) + \frac{\Delta t_i}{2}\,\frac{d^2\vec{x}}{dt^2}\bigg|_{t_i} + O(\Delta t_i^2),
\end{equation}
which implies an LTE that starts at $O(\Delta t_i^3)$. While more accurate, the method is still conditionally stable, though its stability region is larger than that of Forward Euler.

\paragraph{Midpoint stability.} On the prototype $\dot{\vec{x}}=A\,\vec{x}$, Midpoint induces the modal update $y_j(t_{i+1})=\big[1+\lambda_j\Delta t_i\,(1+\tfrac{\lambda_j\Delta t_i}{2})\big]y_j(t_i)$. Stability requires
\begin{equation}
    \big|1+\lambda_j\Delta t_i\,(1+\tfrac{\lambda_j\Delta t_i}{2})\big|^2 \le 1.
\end{equation}
In the complex plane of $z=\lambda\Delta t$, the boundary intersects the imaginary axis at $z=\pm i\sqrt{3}$.

The Forward Euler and Midpoint methods are part of a larger family called \textbf{Runge-Kutta methods}. An $s$-stage explicit Runge--Kutta method has the general form
\begin{align}
    \vec{x}(t_{i+1}) &= \vec{x}(t_i) + \Delta t_i\sum_{j=1}^{s} b_j\,\vec{k}_j, \\
    \vec{k}_j &= \vec{f}\Big(\vec{x}(t_i) + \Delta t_i\sum_{k=1}^{j-1} a_{jk}\,\vec{k}_k,\; t_i + c_j\,\Delta t_i\Big),\quad j=1,\dots,s,
\end{align}
with $\sum_{k=1}^{j-1} a_{jk} = c_j$ for $j\ge 2$. The coefficients are chosen to minimize the LTE by matching terms in the Taylor series; with an optimal choice the LTE scales as $O(\Delta t_i^{s+1})$.

\paragraph{The classical RK4 method.} The widely used four-stage, fourth-order Runge--Kutta (RK4) method updates via
\begin{align}
    \vec{k}_1 &= \vec{f}(\vec{x}_i, t_i), \\
    \vec{k}_2 &= \vec{f}\!\left(\vec{x}_i + \tfrac{\Delta t_i}{2}\,\vec{k}_1,\; t_i + \tfrac{\Delta t_i}{2}\right), \\
    \vec{k}_3 &= \vec{f}\!\left(\vec{x}_i + \tfrac{\Delta t_i}{2}\,\vec{k}_2,\; t_i + \tfrac{\Delta t_i}{2}\right), \\
    \vec{k}_4 &= \vec{f}\!\left(\vec{x}_i + \Delta t_i\,\vec{k}_3,\; t_i + \Delta t_i\right), \\
    \vec{x}_{i+1} &= \vec{x}_i + \tfrac{\Delta t_i}{6}\,(\vec{k}_1 + 2\vec{k}_2 + 2\vec{k}_3 + \vec{k}_4).\label{eq:rk4}
\end{align}
RK4 has LTE $O(\Delta t_i^5)$ and global error $O(\Delta t_i^4)$, at the cost of four right-hand-side evaluations per step. Its simplicity and accuracy make it a common baseline for non-stiff problems. See \cite{DormandPrince1980,ShampineReichelt1997} for adaptive higher-order explicit methods.

\begin{exampleBox}
    \textbf{Exercise (mechanics of RK4)}: For $x'=-2x$, $x(0)=1$, take one RK4 step with $\Delta t=0.1$ at $t=0$. Compute $k_1,\dots,k_4$ and $x(0.1)$ from \autoref{eq:rk4}. Compare with the exact $x(t)=e^{-2t}$.
\end{exampleBox}

\subsubsection{Comparing explicit methods on a test problem}
For $\dot{y}=-y$ with $y(0)=1$ and terminal time $t=1$, the global error of Forward Euler scales proportionally to $\Delta t$ while the Midpoint method scales proportionally to $\Delta t^2$. As the step size decreases, the Midpoint method achieves a given error tolerance with far fewer steps. Stability behavior also differs at larger step sizes: Forward Euler loses stability once $\Delta t$ exceeds twice the characteristic time, whereas the Midpoint method admits a larger stability region but can introduce sign alternation for sufficiently large negative real $\lambda\Delta t$.

\subsection{Implicit Methods}
\label{subsec:ivp-implicit}
The conditional stability of explicit methods poses a major challenge for certain types of problems. \textbf{Implicit methods} provide a powerful alternative by using information about the \textit{unknown} future state, $\vec{x}(t_{i+1})$, to calculate the step. The general form for an implicit method is:
\begin{equation}
    \vec{x}(t_{i+1}) = \vec{x}(t_i) + \Delta t_i \vec{g}(\vec{x}(t_{i+1}), \vec{x}(t_i), t_{i+1}, t_i, \Delta t_i)
\end{equation}
Because $\vec{x}(t_{i+1})$ appears on both sides of the equation, each step requires solving a system of equations (often nonlinear) to find the next state. This makes each step more computationally expensive than an explicit step. However, this trade-off is often justified by their superior stability properties.

\subsubsection{Backward Euler Method}
The simplest implicit method is the \textbf{Backward Euler} method. It is analogous to the Forward Euler method but uses the slope at the end of the interval, $\vec{f}(\vec{x}(t_{i+1}), t_{i+1})$, to take the step. It is also analogous to a right Riemann sum for integration.
\begin{equation}
    \vec{x}(t_{i+1}) = \vec{x}(t_i) + \Delta t_i \vec{f}(\vec{x}(t_{i+1}), t_{i+1})
    \label{eq:backward_euler}
\end{equation}
To find $\vec{x}(t_{i+1})$, we must solve the (generally nonlinear) algebraic equation $\vec{y} - \Delta t_i \vec{f}(\vec{y}, t_{i+1}) - \vec{x}(t_i) = 0$ for $\vec{y} = \vec{x}(t_{i+1})$. This is typically done using an iterative method like Newton's method, which requires computing the Jacobian of the system. The Backward Euler method has a local truncation error of $O(\Delta t_i^2)$ and a global error of $O(\Delta t_i)$, the same as Forward Euler.

The key advantage of Backward Euler is its stability. Applying it to the prototype problem gives:
\begin{align}
    x(t_{i+1}) &= x(t_i) + \Delta t \lambda x(t_{i+1}) \\
    (1 - \lambda \Delta t)x(t_{i+1}) &= x(t_i) \\
    x(t_{i+1}) &= (1 - \lambda \Delta t)^{-1} x(t_i)
\end{align}
The stability condition is now $|(1 - \lambda \Delta t)^{-1}| \le 1$. This inequality holds for any $\Delta t > 0$ as long as $\mathrm{Re}(\lambda) \le 0$. Equivalently,
\begin{equation}
    |1 - \lambda\Delta t|^2 = (1-\mathrm{Re}\,\lambda\,\Delta t)^2 + (\mathrm{Im}\,\lambda\,\Delta t)^2 \ge 1.
\end{equation}
The stability region covers the entire left half of the complex plane. Methods with this property are called \textbf{A-stable}, meaning they are unconditionally stable for any stable linear system.

\paragraph{Stability functions and L-stability.} Any one-step method applied to $x'=\lambda x$ induces $x_{i+1}=R(z)\,x_i$ with $z=\lambda\Delta t$, where $R$ is the method's \emph{stability function}. For Backward Euler $R(z)=(1-z)^{-1}$; for Crank--Nicolson $R(z)=\tfrac{1+z/2}{1-z/2}$. A method is \emph{A-stable} if $|R(z)|\le 1$ for all $\mathrm{Re}(z)\le 0$. It is \emph{L-stable} if it is A-stable and additionally $\lim_{z\to -\infty} R(z)=0$, which ensures strong damping of very fast stable modes. Backward Euler is L-stable, whereas Crank--Nicolson is A-stable but not L-stable; the latter can exhibit nonphysical oscillations for very stiff decay.

\subsubsection{Crank-Nicolson Method}
The \textbf{Crank-Nicolson method} improves upon the accuracy of the Euler methods by averaging the slopes at the beginning and end of the interval. This is equivalent to using the trapezoidal rule for integration.
\begin{equation}
    \vec{x}(t_{i+1}) = \vec{x}(t_i) + \frac{\Delta t_i}{2} [\vec{f}(\vec{x}(t_i), t_i) + \vec{f}(\vec{x}(t_{i+1}), t_{i+1})]
    \label{eq:crank_nicolson}
\end{equation}
This method is also implicit and requires solving an algebraic system at each step. Its local truncation error is $O(\Delta t_i^3)$, making it second-order accurate globally ($O(\Delta t_i^2)$). Like the Backward Euler method, Crank-Nicolson is A-stable and thus suitable for problems where stability, rather than just accuracy, is a primary concern.

A short expansion confirms the $O(\Delta t_i^3)$ local error. Using $\vec{f}(\vec{x}(t_{i+1}),t_{i+1})=\vec{f}(\vec{x}(t_i),t_i)+\Delta t_i\,\tfrac{d\vec{f}}{dt}|_{(\vec{x}(t_i),t_i)}+O(\Delta t_i^2)$ and substituting into the average yields
\begin{equation}
    \frac{1}{2}\Big[\vec{f}(\vec{x}(t_i),t_i)+\vec{f}(\vec{x}(t_{i+1}),t_{i+1})\Big] = \vec{f}(\vec{x}(t_i),t_i) + \frac{\Delta t_i}{2}\,\frac{d\vec{f}}{dt}\bigg|_{(\vec{x}(t_i),t_i)} + O(\Delta t_i^2) = \vec{f}(\vec{x}(t_i),t_i) + \frac{\Delta t_i}{2}\,\frac{d^2\vec{x}}{dt^2}\bigg|_{t_i} + O(\Delta t_i^2),
\end{equation}
and comparison with the Taylor expansion of $\vec{x}_{\text{exact}}(t_{i+1})$ shows the leading LTE term is $O(\Delta t_i^3)$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/ode_ivp/crank_nicolson_stability_region.pdf}
    \caption{Stability region for the Crank--Nicolson method: $|R(z)|\le 1$ for $\mathrm{Re}(z)\le 0$, with $R(z)=\tfrac{1+z/2}{1-z/2}$. CN is A-stable but not L-stable.}
    \label{fig:cn-stability}
\end{figure}

\subsubsection{Implicit stability on $\dot{y}=-y$}
For the scalar decay $\dot{y}=-y$ with $y(0)=1$, Backward Euler produces a monotone decay to zero for any $\Delta t>0$, while Crank--Nicolson remains stable yet may exhibit mild oscillations in very stiff regimes due to its non-L-stability. For moderate $\Delta t$ values such as $\Delta t\in\{0.1,0.5,1,2\}$, both methods track the exact $e^{-t}$ qualitatively, with Crank--Nicolson showing higher accuracy at the same $\Delta t$.

\subsubsection{Connection to numerical integration}
Writing $\tfrac{d}{dt}\,\vec{x}(t)=\vec{f}(\vec{x}(t),t)$ and integrating from $t_0$ to $t$ gives $\vec{x}(t)=\vec{x}_0+\int_{t_0}^{t}\vec{f}(\vec{x}(\tau),\tau)\,d\tau$. When $\vec{f}$ depends only on time, this reduces to $\vec{x}(t)=\vec{x}_0+\int_{t_0}^{t}\vec{f}(\tau)\,d\tau$. Forward Euler corresponds to a left Riemann sum, Backward Euler to a right Riemann sum, and Crank--Nicolson to the trapezoidal rule.

\subsubsection{Solving implicit steps via Newton's method}
Each implicit step defines a nonlinear residual $\vec{r}(\vec{y}) = \vec{y} - \vec{x}(t_i) - \Delta t_i\,\vec{f}(\vec{y}, t_{i+1})$. Newton's method seeks $\vec{r}(\vec{y})=\vec{0}$ by iterates
\begin{equation}
    \left[I - \Delta t_i\,\nabla_{\!x} \vec{f}(\vec{y}^{(k)}, t_{i+1})\right] \Delta \vec{y}^{(k)} = -\vec{r}(\vec{y}^{(k)}),\quad \vec{y}^{(k+1)}=\vec{y}^{(k)}+\Delta \vec{y}^{(k)}.
\end{equation}
For large systems, Jacobian-free inexact Newton--Krylov methods are effective; damping or line search improves robustness. The linear system inherits stiffness, so preconditioning is often decisive \cite{HairerWannerII,Hindmarsh2005SUNDIALS}.

\begin{exampleBox}
    \textbf{Exercise (implicit Newton step)}: For the scalar ODE $x'=-x^3$, Backward Euler gives $y - x_i + \Delta t\, y^3=0$. With $x_i=1$, $\Delta t=0.1$, and initial guess $y^{(0)}=0.9$, compute one Newton update $y^{(1)}$.
\end{exampleBox}

\subsubsection{Backward Differentiation Formulas (BDF)}
Multi-step implicit BDF methods are the workhorses for stiff problems. They approximate the time derivative by a backward polynomial through the last $k$ solution points, yielding order-$k$ formulas with stiff decay. Orders $k\le 2$ are A-stable; higher orders are $A(\alpha)$-stable and remain effective in practice with adaptive order/step control. Widely used solvers (e.g., MATLAB's \texttt{ode15s}, SUNDIALS CVODE) implement variable-order BDF with Nordsieck representation and WRMS error control \cite{Gear1971BDF,ShampineReichelt1997,Hindmarsh2005SUNDIALS}.

\subsubsection{Mass-matrix ODEs and index-1 DAEs}
Many models appear in the semi-implicit form
\begin{equation}
    M(\vec{x},t)\,\frac{d\vec{x}}{dt} = \vec{f}(\vec{x},t),\label{eq:mass-matrix-ode}
\end{equation}
with a (possibly state-dependent) capacity or mass matrix $M$. When $M$ is singular but constraints are algebraic and well-posed, \autoref{eq:mass-matrix-ode} defines an index-1 DAE. Chemical engineering sources include method-of-lines discretizations of diffusion/energy balances (nontrivial heat capacities/volumes), equilibrium constraints, and flows with pressure algebraics. Modern implicit solvers accept $M$ directly and treat DAEs with consistent initialization.

\subsection{The linear prototype revisited}
\label{subsec:prototype-revisited}
The linear initial value problem $\dot{\vec{x}}=A\,\vec{x}$ with $\vec{x}(0)=\vec{x}_0$ on $t\in[0,t_f]$ admits a closed-form solution when $A$ has a complete set of eigenvectors. With $A=W^{-1}\Lambda W$, the exact solution is $\vec{x}(t)=W^{-1}e^{\Lambda t}W\,\vec{x}_0$, and the modal components evolve as $y_i(t)=y_i(0)\,e^{\lambda_i t}$. Analytical stability holds when $\mathrm{Re}(\lambda_i)\le 0$ for all $i$. The dynamics are characterized by time constants $\tau_i=|\lambda_i^{-1}|$. When the ratios $\tfrac{\tau_{\max}}{\tau_{\min}}$ or $\tfrac{t_f}{\tau_{\min}}$ are large, the solution spans a wide range of time scales and capturing details across all scales becomes important.

\subsection{Stiffness and its Implications}
\label{subsec:ivp-stiffness}
The choice between explicit and implicit methods often comes down to whether the ODE system is \textbf{stiff}. Stiffness is a subtle property that is easy to recognize but difficult to formally define.

\subsubsection{Understanding Stiff Systems}
A system is generally considered stiff if it involves physical phenomena that occur on widely different time scales. In the context of our linear prototype problem, the time scale of a component is related to the inverse of its corresponding eigenvalue, $\tau_i = |\lambda_i^{-1}|$. A system is stiff if the \textbf{stiffness ratio}, the ratio of the slowest time scale to the fastest time scale, is large:
\begin{equation}
    \text{Stiffness Ratio} = \frac{\tau_{\text{max}}}{\tau_{\text{min}}} = \frac{\max_i |\text{Re}(\lambda_i)|}{\min_i |\text{Re}(\lambda_i)|} \gg 1
\end{equation}
The problem with stiff systems is the behavior of explicit solvers. The stability of an explicit method is governed by the fastest time scale (largest $|\mathrm{Re}(\lambda)|$), requiring $\Delta t$ to be on the order of $\tau_{\min}$. However, the dynamics of interest may be evolving on the much slower time scale of $\tau_{\max}$. The explicit solver is thus forced to take tiny steps to maintain stability, even after the fast transient components have decayed and the solution is changing very slowly. This leads to an enormous number of steps, excessive computation time, and the potential for accumulating round-off error. The practical conflict between maintaining numerical stability and accuracy with very small steps and avoiding the accumulation of round-off error over many steps is the essence of stiffness. There are no A-stable explicit one-step methods, which is why implicit methods are favored in this regime.

Implicit solvers, being A-stable, do not have this stability restriction. The step size for an implicit solver is constrained only by the need for accuracy, not stability. Therefore, they can take much larger time steps for stiff systems, making them far more efficient.

\subsubsection{Choosing the Right Solver: A Practical Example}
The Van der Pol oscillator is a classic example of a system that becomes stiff for certain parameter values. The governing equations are:
\begin{align}
    \frac{dx_1}{dt} &= x_2 \\
    \frac{dx_2}{dt} &= \mu(1 - x_1^2)x_2 - x_1
\end{align}
When the parameter $\mu$ is large (e.g., $\mu=1000$), the system becomes very stiff. The solution exhibits periods of very slow change punctuated by extremely rapid transitions. A linearization around a state $\vec{x}^{\,*}=(x_1^{\,*},x_2^{\,*})$ gives a Jacobian
\begin{equation}
    J(\vec{x}^{\,*}) = \begin{pmatrix} 0 & 1 \\ -2\mu x_1^{\,*}x_2^{\,*} - 1 & \mu\big(1-(x_1^{\,*})^2\big) \end{pmatrix},
\end{equation}
so that near $x_1^{\,*}=0$ the linearized system behaves like $\big(\begin{smallmatrix}0&1\\-1&\mu\end{smallmatrix}\big)$ with time scales $\tau_{\min}\approx 1/\mu$ and $\tau_{\max}\approx \mu$. At points where $x_2^{\,*}=0$, the matrix $\big(\begin{smallmatrix}0&1\\-1&\mu(1-(x_1^{\,*})^2)\end{smallmatrix}\big)$ implies $\tau_{\max}\approx \mu\big(1-(x_1^{\,*})^2\big)$ and $\tau_{\min}\approx 1/\big(\mu\big(1-(x_1^{\,*})^2\big)\big)$, leading to a stiffness ratio that grows like $O(\mu^2)$.

\begin{warningBox}
    \textbf{A Tale of Two Solvers}: A comparison of a standard explicit Runge--Kutta solver like MATLAB's \texttt{ode45} and a solver designed for stiff problems like MATLAB's \texttt{ode15s} is illustrative. For the Van der Pol oscillator with $\mu=1000$, a typical explicit run requires about $1.7\times 10^6$ successful steps and $1.1\times 10^7$ function evaluations taking roughly 19 seconds, whereas an implicit run completes in 595 successful steps with 1916 function evaluations taking about 0.06 seconds. The explicit solver is forced to take millions of tiny steps to remain stable through the rapid transitions, while the implicit solver handles them efficiently with large steps. This demonstrates the importance of selecting an appropriate solver for stiff systems.
\end{warningBox}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/ode_ivp/vdp_stiff_compare.pdf}
    \caption{Van der Pol oscillator with $\mu=100$: internal step locations for an explicit RK method (blue) versus an implicit BDF method (red). The explicit method takes many tiny steps during fast transitions; the implicit method adapts efficiently.}
    \label{fig:vdp-stiff}
\end{figure}

\noindent\textit{Implementation sketch and performance on MATLAB-like solvers.} The following pseudocode outlines a typical setup for comparing an explicit and an implicit solver on the Van der Pol oscillator when the stiffness parameter is moderate to large:
\begin{verbatim}
clear;
f = @(x, t) [x(2); mu*(1 - x(1).^2).*x(2) - x(1)];
options = odeset('stats','on');
[t45, x45] = ode45(@(t, x) f(x, t), [0, 100*pi], [2; 0], options);
[t15, x15] = ode15s(@(t, x) f(x, t), [0, 100*pi], [2; 0], options);
\end{verbatim}
For $\mu=100$, a representative explicit run reports 17383 successful steps, 1100 failed attempts, and 110899 function evaluations with an elapsed time of approximately 0.21 seconds, while an implicit run reports 455 successful steps, 161 failed attempts, 1422 function evaluations, 32 partial derivatives, 211 LU decompositions, and 1325 linear solves with an elapsed time of approximately 0.043 seconds. For a milder case with $\mu=10$, the explicit run reports 228 successful steps, 32 failed attempts, and 1561 function evaluations with an elapsed time near 0.008 seconds, whereas the implicit run reports 327 successful steps, 77 failed attempts, 856 function evaluations, 9 partial derivatives, 113 LU decompositions, and 828 linear solves with an elapsed time near 0.033 seconds. For a very stiff case with $\mu=1000$, the explicit run reports approximately $1.73388\times 10^6$ successful steps, 115515 failed attempts, and $1.10963\times 10^7$ function evaluations taking about 19.03 seconds, while the implicit run reports 595 successful steps, 231 failed attempts, 1916 function evaluations, 47 partial derivatives, 295 LU decompositions, and 1774 linear solves with an elapsed time of about 0.059 seconds.

\subsection{Advanced Topic: Adaptive Time Stepping}
\label{subsec:ivp-adaptive-time-stepping}
In practice, a constant step size $\Delta t$ is inefficient. We would ideally take small steps when the solution is changing rapidly and large steps when it is changing slowly. This is the principle behind \textbf{adaptive time stepping}.

The core idea is to estimate the local truncation error at each step and adjust $\Delta t$ to keep this error within a specified tolerance. One common way to do this is to use an \textbf{embedded method}. For example, a popular Runge--Kutta method known as \texttt{ode45} (Dormand--Prince) computes two solutions at each step: one with fourth-order accuracy and another with fifth-order accuracy. The difference estimates the LTE of the lower-order method. A control loop accepts or rejects the step and chooses a new $\Delta t$ to meet the tolerance \cite{DormandPrince1980,ShampineReichelt1997}.

Error control is performed in a weighted norm. With absolute and relative tolerances $(\mathrm{atol}_i,\,\mathrm{rtol})$, define the WRMS norm
\begin{equation}
    \|\vec{e}\|_{\mathrm{WRMS}} = \sqrt{\frac{1}{n}\sum_{i=1}^n \left(\frac{e_i}{\mathrm{atol}_i + \mathrm{rtol}\,\max\{ |x_i|, |\hat{x}_i|\}}\right)^2},
\end{equation}
where $\hat{\vec{x}}$ is the trial solution. A typical proportional-integral controller for a method of order $p$ updates
\begin{equation}
    \Delta t_{\mathrm{new}} = \Delta t\;\mathrm{safety}\;\left(\frac{1}{\|\vec{e}\|_{\mathrm{WRMS}}}\right)^{\alpha}\left(\frac{1}{\|\vec{e}_{\mathrm{prev}}\|_{\mathrm{WRMS}}}\right)^{\beta},\quad \alpha=\frac{1}{p+1},\; \beta=\frac{\alpha}{4},
\end{equation}
with bounds $\Delta t_{\min}\le \Delta t_{\mathrm{new}}\le \Delta t_{\max}$ and a safety factor (e.g., 0.8). Steps with $\|\vec{e}\|_{\mathrm{WRMS}}\le 1$ are accepted; otherwise they are retried with a smaller step. Dense output provides accurate interpolation between accepted steps.

An alternative error estimator can be constructed even for a single two-stage method. For the Midpoint method, the three fluxes $\vec{f}(\vec{x}(t_i),t_i)$, $\vec{f}(\vec{y},t_i+\tfrac{\Delta t_i}{2})$, and $\vec{f}(\vec{x}(t_{i+1}),t_{i+1})$ can be combined to form
\begin{equation}
    \Delta t_i\,\Big[\vec{f}(\vec{x}(t_i),t_i) + \vec{f}(\vec{x}(t_{i+1}),t_{i+1}) - 2\,\vec{f}\big(\vec{y},t_i+\tfrac{\Delta t_i}{2}\big)\Big] \sim O(\Delta t_i^3),
\end{equation}
which serves as a proxy for the LTE. A practical acceptance test is
\begin{equation}
    \big\|\Delta t_i\,[\,\vec{f}(\vec{x}(t_i),t_i) + \vec{f}(\vec{x}(t_{i+1}),t_{i+1}) - 2\,\vec{f}(\vec{y},t_i+\tfrac{\Delta t_i}{2})\,]\big\|_2 \le \varepsilon_a + \varepsilon_r\,\big\|\vec{f}(\vec{x}(t_i),t_i)\big\|_2.
\end{equation}
If the inequality fails, the step is rejected and $\Delta t_i$ is reduced; if it succeeds comfortably, the step is accepted and $\Delta t_i$ is increased for efficiency. Similar ideas underpin adaptive controllers for many IVP solvers.

\begin{exampleBox}
    \textbf{Exercise (step-size control)}: Suppose an embedded pair of order $p=4$ yields $\|\vec{e}\|_{\mathrm{WRMS}}=0.5$ and the previous was 0.8. With safety 0.9, compute $\Delta t_{\mathrm{new}}/\Delta t$ using the PI rule above, then apply bounds $[0.2,5]$.
\end{exampleBox}

\begin{figure}[h!]
    \centering
    % TODO: Create figure for adaptive controller behavior.
    % Suggested content: Step-size evolution $\Delta t(t)$ and accepted/rejected steps for a solution with fast/slow phases (e.g., Van der Pol).
    \includegraphics[width=0.8\textwidth]{figs/placeholder.pdf}
    \caption{Illustration of adaptive step-size control: smaller steps during rapid transients and larger steps on slow manifolds.}
    \label{fig:adaptive-controller}
\end{figure}

\subsection*{Additional chemical engineering examples}
\begin{itemize}
    \item \textbf{Method-of-lines heat conduction}: Discretize $T_t = \alpha T_{xx}$ on a slab; the semi-discrete system is stiff with eigenvalues $\sim -\alpha\pi^2/h^2$. Compare explicit RK4 and Backward Euler at equal error.
    \item \textbf{CSTR with Arrhenius kinetics}: Coupled $C$ and $T$ balances exhibit rapid thermal transients and slow approach to steady state; demonstrate stiffness and solver selection.
    \item \textbf{Plug-flow reactor}: Axial discretization of convection--reaction yields a large ODE system; discuss mass-matrix forms when using finite elements.
\end{itemize}

\begin{exampleBox}
    \textbf{Exercise (stiffness ratio)}: For a second-order central-difference discretization of $y_t=D y_{xx}$ on $[0,1]$ with $N$ interior nodes and Dirichlet boundaries, approximate the eigenvalues of the semi-discrete operator and estimate the stiffness ratio as a function of $N$.
\end{exampleBox}